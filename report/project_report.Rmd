---
title: 'BIOS 735 Final Project: US Car Accidents and Traffic Severity'
author: "Marissa Ashner, Yen Chang, Marco Chen, Yi Tang Chen, Weifang Liu, Joyce Yan"
date: "4/27/2020"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_float: true
    theme: readable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(png)
library(grid)
library(kableExtra)
library(ggplot2)
library(ordinalNet)
library(dplyr)
```

# Introduction 

Sitting in traffic is not ideal for anyone, and it is especially frustrating when the traffic is caused by an accident. It is an unexpected delay, and many times it is completely uncertain how long the delay will last.  For a country where the majority of the population relies so heavily on automobiles for daily transportation, traffic delays may result in a multitude of disruptions in many other aspects of life as well.  

Not only is sitting traffic frustrating, but the resulting air pollution and long-term exposure to vehicle emissions may also be associated with future health and respiratory issues.<sup>[1]</sup>  Moreover, traffic accidents pose a big problem for public safety. In many studies, traffic congestion has been considered a potentially important factor impacting the occurrence of accidents,<sup>[2]</sup> suggesting that the severity of a traffic delay caused by one accident may influence the likelihood of a secondary accident.

Traffic accident data can therefore be an invaluable resource, as it can provide information that can potentially aid in predicting conditions where traffic accidents are prone to happen, so that steps can be taken to prevent the accident. Further, this data could also be useful in predicting how severely an accident will delay the drivers following the accident, so that steps can be taken to reduce this delay as needed. 

Several computer scientists from the Ohio State University present exactly this: a dataset of traffic accidents from the contiguous United States, covering 2.95 million accidents with data collected between February 2016 and December 2019. As of March 2019, of the 49 states included, North Carolina had the fourth highest number of accidents in the dataset at 109,000. <sup>[3]</sup> Because of the wide range of attributes collected with each accident, this dataset has the potential to provide the information needed to start making decisions about how to mitigate traffic accidents and the delays associated.

# Project Aim 

The aim of this project is to use Car Accident Data from North Carolina in 2016-2018 to determine which factors have the most influence on the severity of car accidents in the state. We take two different approaches to construct models to predict the severity of car accidents, one through a proportional odds model and the other through machine learning Forest-based methods. We then use the 2019 data to evaluate and compare the performance of our prediction models.

# R-Package

Our R package `group3project` contains the functions we wrote needed to fit a proportional odds model as well as the datasets we used to do so for this report. You can find this package on [GitHub](https://github.com/kennyitang/BIOS735-Group3/tree/master/group3project). We provide more details on these functions in the later sections.

* Functions for Proportional Odds Model
    + `pom.est`: return the estimates and optionally the standard errors of the intercept and slope estimates of a proportional odds model. It calls the following functions:
        - `loglik.pom`: computes log likelihood of a proportional odds model, 
        - `gradient.pom`: computes the gradient of the log likelihood of a proportional odds model,
        - `pom.logistic`: evaluates a standard logistic function/anti-logit function, and
        - `pom.recode`: which prepares the design matrix for the proportional odds model.

* Functions for Random Forest
    + `calc_acc`: calculates the accuracy of classification.

* Datasets 
    + `tst_data`: Testing Data
    + `trn_data`: Training Data
    + `nc_data`: Full NC Data

```{r, message = FALSE}
## load our package and datasets here
devtools::load_all("../group3project")
data("trn_data")
data("tst_data")

# run the following code to see documentation 
# devtools::install("../group3project")
# library(group3project)
```

# Data
The original dataset had 49 total attributes, including attributes related to traffic, location, weather, points of interest, and time of day.  Pre-processing the data set was necessary to allow us to focus on the variables and observations of real relevance to our aim. Although we started out with 48 independent variables and one outcome variable, there were several reasons for removing most of the attributes, including:  

1. The attribute had at least 60% missingness.
    + Wind Chill (which is also redundant since it is a function of Temperature and Wind speed) and Precipitation (which also has low variability)
2. The attribute had very low or no variability across the observations.  
    + After aggregating the County variable into urban and rural factor levels, it was apparent that about 95% of the accident data was derived from urban areas, therefore variables such as County, Zip-Code, and City were not of use 
    + Most of the points of interest variables except Crossing and Traffic Signal
3. The attribute was relevant for spatial purposes (i.e. latitude and longitude), which we do not aim to study at this time.  
  
In terms of individual observations, rows missing all weather variables were removed. After pre-processing, the analysis dataset contains 119,279 traffic accidents from North Carolina between the years 2016 and 2019 with 12 attributes and one outcome variable. Additionally, the analysis data were split up into training data (pre-2019) and testing data (2019) as mentioned previously. The final training dataset has 75,887 observations and the testing has 43,392 observations.

If you’re interested in looking at the code for the Data Processing and Exploratory Analysis, please see `data_prep.R` and `exploratory_analysis.R` in the *scripts* folder of our [GitHub repository](https://github.com/kennyitang/BIOS735-Group3/tree/master/scripts). 
  
The variables selected for analysis, and some accompanying exploratory plots are as follows: 

### Outcome:
1. Severity:  severity of the accident in terms of the impact on traffic, which is an integer from 1 to 3 with increasing impact. This is an ordinal variable and the outcome of interest.  

Originally this variable had four categories (1-4), but only 24 observations had severity of 1, so we combined severities 1 and 2 and readjusted the scale. Even after this aggregation, our dataset is still very unbalanced. As seen in the code output below, severity class 1 has many more accidents in the training data than the more severe classes.

```{r}
trn_data$Severity_c = as.factor(trn_data$Severity)
tst_data$Severity_c = as.factor(tst_data$Severity)
table(trn_data$Severity_c)
```

It should be noted that the data set sources are rather vague on the definition of this outcome variable. Length of delay caused by the accident is the major determinant, and we assume that scope, such as how many lane closures there were, may also be taken into account when determining the severity. 

### Predictors:

1. Source: categorical, API that reported accident (MapQuest <sup>[5]</sup>, Bing <sup>[6]</sup>, MapQuest-Bing)  
    + We would expect an intersection between the data from different sources
    + Both APIs have severity scales 1-4, but we don't know that they are harmonious
    + Including Source Predictor in our models to determine whether or not it is important in predicting our outcome 

**Location/Time** 

2. Side: categorical, side of street (Left/Right)  
3. Sunrise_Sunset: categorical (Day/Night)
4. Weekday: TRUE/FALSE, if accident started on a weekday 
5. Interstate: TRUE/FALSE, if accident happened on an interstate
  
**Weather**  

6. Temperature: numeric, degrees Fahrenheit    
7. Humidity: numeric, percentage  
8. Pressure: numeric air pressure, in inches  
9. Visibility: numeric, in miles  
10. Wind Speed: numeric, in mph 

```{r fig.width=8,echo=FALSE}
grid.raster(readPNG("weather_boxplots.png"))
```
  
**Points of Interest**  

11. Crossing: TRUE/FALSE, presence of a crossing nearby for pedestrians, cyclists, etc. 
12. Traffic_Signal: TRUE/FALSE, presence of a traffic signal nearby on an intersection 

```{r,fig.width=10, echo=FALSE}
img1 <-  grid::rasterGrob(as.raster(readPNG("crossing2.png")),
                            interpolate = FALSE)
img2 <-  grid::rasterGrob(as.raster(readPNG("traffic_signal2.png")),
                          interpolate = FALSE)
gbm::grid.arrange(img1, img2, ncol = 2)
```

# Methods 
We apply a proportional odds model and a penalized version with severity of an accident as the outcome, and predictors listed in the previous section as covariates. We identify variables that are most associated with severity using Step-AIC. In parallel, starting with the same set of variables, we use random forest and ordinal forest to find the most important predictors and build a model that yields the best predictive performance.  

## Proportional Odds Model  
For a single severity outcome $Y_i \in \{1,2,3\}$ with covariates $x_i \in \mathbb{R}^p$, the usual proportional odds model suggests that 
\[
  {\displaystyle
    \begin{aligned}
      logit(P(Y_i\leq k))=log(\frac{P(Y_i\leq k)}{1-P(Y_i\leq k)}) = \alpha_k - x_i^T\beta\\
      &\qquad\qquad\qquad &\qquad\qquad\qquad &\qquad\qquad\qquad &\qquad\qquad\qquad &\qquad\qquad\qquad &\qquad\qquad\qquad &\qquad\qquad
    \end{aligned}
  }
\], where \(\beta = (\beta_1,...,\beta_p)^T, x_i = (x_{i1},..., x_{ip})^T,\) and \(k= 1, 2\).

<br>

Equivalently, if we define \(\phi(t) = \frac{1}{1+exp(-t)}\), then the model can be written as
\[  
{\displaystyle
    \begin{aligned}
P(Y_i\leq k) = \frac{1}{1 + exp(x_i^T\beta-\alpha_k)} = \phi(\alpha_k-x_i^T\beta).&\\
                                   & \qquad\qquad\qquad &\qquad\qquad\qquad &\qquad\qquad\qquad &\qquad\qquad\qquad &\qquad\qquad\ 
    \end{aligned}
  }  
\]

If we define $\alpha = (\alpha_1,\alpha_2)^T$, then log likelihood function for n accidents is therefore given by <sup>[7]</sup> 
\[\small
  {\displaystyle
    \begin{aligned}
l(\alpha, \beta)&= \sum_{i=1}^n\sum_{k=1}^3I(Y_i=k)log(P(Y_i=k))\\
                &=\sum_{i=1}^n \{I(Y_i=1)log(P(Yi\le1))+ I(Y_i=2)[log(P(Yi\le2)-P(Y_i\le1)] + I(Y_i=3)[log(1-P(Y_i\le2)]\}\\
                &=\sum_{i=1}^n\{I(Y_i=1)log(\phi(\alpha1-x_i^T\beta))+ I(Y_i=2)log(\phi(\alpha2-x_i^T\beta)-\phi(\alpha1-x_i^T\beta))+  I(Y_i=3)log(1-\phi(\alpha2-x_i^T\beta))\}
    \end{aligned}
  }
\]

The gradient is 
\[
  {\displaystyle
    \begin{aligned}
(\frac{\partial l(\alpha, \beta)}{\partial \beta_1}, ..., \frac{\partial l(\alpha, \beta)}{\partial \beta_p}, \frac{\partial l(\alpha, \beta)}{\partial \alpha_1}, \frac{\partial l(\alpha, \beta)}{\partial \alpha_2})^T&\\
                                   & \qquad\qquad\qquad &\qquad\qquad\qquad &\qquad\qquad\qquad & 
    \end{aligned}
  }
\]

, with the individual elements given below.

<br>

\[\small
  {\displaystyle
    \begin{aligned}
        \frac{\partial l(\alpha, \beta)}{\partial \beta_j}
          &= \sum_{i=1}^n\left(-I(Y_i=1)\frac{exp(x_i^T\beta-\alpha_1)}{1+exp(x_i^T\beta-\alpha_1)}x_{ij}
            + I(Y_i=2)\frac{\frac{exp(x_i^T\beta-\alpha_1)}{[1+exp(x_i^T\beta-\alpha_1)]^2}-
                \frac{exp(x_i^T\beta-\alpha_2)}{[1+exp(x_i^T\beta-\alpha_2)]^2}}{\phi(\alpha_2-x_i^T\beta)-\phi(\alpha_1-x_i^T\beta)}x_{ij} 
            + I(Y_i=3)\frac{x_{ij}}{1+exp(x_i^T\beta-\alpha_2)}\right)
          \\&=\sum_{i=1}^n\left(-I(Y_i=1)(1-\phi(\alpha_1-x_i^T\beta)) 
            + I(Y_i=2)\frac{\phi(\alpha_1-x_i^T\beta)[1-\phi(\alpha_1-x_i^T\beta)]
              -\phi(\alpha_2-x_i^T\beta)[1-\phi(\alpha_2-x_i^T\beta)]}{\phi(\alpha_2-x_i^T\beta)-\phi(\alpha_1-x_i^T\beta)} 
            +I(Y_i=3)\phi(\alpha_2-x_i^T\beta)\right)x_{ij}\\
        \\
        \frac{\partial l(\alpha, \beta)}{\partial \alpha_1} 
        &= \sum_{i=1}^n \left(I(Y_i=1)\frac{exp(x_i^T\beta-\alpha_1)}{1+exp(x_i^T\beta-\alpha_1)}
          -I(Y_i=2)\frac{\frac{exp(x_i^T\beta-\alpha_1)}{[1+exp(x_i^T\beta-\alpha_1)]^2}}{\phi(\alpha_2-x_i^T\beta)-\phi(\alpha_1-x_i^T\beta)} \right)\\
        &=\sum_{i=1}^n\left( I(Y_i=1)[1-\phi(\alpha_1-x_i^T\beta)]
          - I(Y_i=2)\frac{\phi(\alpha_1-x_i^T\beta)[1-\phi(\alpha_1-x_i^T\beta)]}{\phi(\alpha_2-x_i^T\beta)-\phi(\alpha_1-x_i^T\beta)} \right)\\
        \\
     \end{aligned}
  }
\]

\[
  {\displaystyle
    \begin{aligned}
        \frac{\partial l(\alpha, \beta)}{\partial \alpha_2} &= \sum_{i=1}^n \left(
            I(Y_i=2)\frac{\frac{exp(x_i^T\beta-\alpha_2)}{[1+exp(x_i^T\beta-\alpha_2)]^2}}{\phi(\alpha_2-x_i^T\beta)-\phi(\alpha_1-x_i^T\beta)}
            -I(Y_i=3)\frac{\frac{exp(x_i^T\beta-\alpha_2)}{[1+exp(x_i^T\beta-\alpha_2)]^2}}{1-\phi(\alpha_2-x_i^T\beta)} \right)
            \\                       
            &=\sum_{i=1}^n\left( I(Y_i=2)\frac{\phi(\alpha_2-x_i^T\beta)[1-\phi(\alpha_2-x_i^T\beta)]}{\phi(\alpha_2-x_i^T\beta)-\phi(\alpha_1-x_i^T\beta)}
            - I(Y_i=3)\frac{\phi(\alpha_2-x_i^T\beta)[1-\phi(\alpha_2-x_i^T\beta)]}{1-\phi(\alpha_2-x_i^T\beta)} \right)      \qquad\qquad\qquad\qquad
  \end{aligned}
  }
\]   

To fit the proportional odds model, we use the `optimx` function with BFGS as the method, and supply the likelihood and gradient functions given above. In our package, the proportional odds model estimation function takes user-defined formula and datasets and automatically span the model matrix for the subsequent computation of likelihood and gradient. Our function checks whether the input dependent variable is a valid ordered factor variable. It uses `optimx`'s second derivative optimality condition check to recommend users to scale continuous variables in order to obtain stable standard error estimates. In contrast to existing proportional odds model functions such as `MASS:polr()`, our function used arbitrary instead of `glm.fit` output as model starting values. While this might increase computation time, the concavity of the proportional odds model ensures the global maximum will be reached regardless of the starting value <sup>[8]</sup>.

### Variable Selection for the proportional odds model

We used the `stepAIC` function in the `MASS` package to perform variable selection for the proportional odds model. We set the direction to be `both` in the `stepAIC` function, which means that we compared the AIC improvements from both dropping and adding each candidate covariate until we reach to the model with the smallest AIC. 
The full model we started with in the variable selection process is the model with 12 predictors that we manually selected in the Data section of the report, with no interactions or higher order polynomial functions of our predictors. 

### Penalized proportional odds model

We also explored fitting a penalized proportional odds model to our data with an elastic penalty by coordinate descent with the `ordinalNet` package. The likelihood and the linear predictor take the same forms as in the proportional odds model. The penalty term is given as $$\lambda\sum_{j=1}^Pc_j\{\alpha|b_j|+\frac{1}{2}(1-\alpha)b_j^2\},$$

where in our case we chose $c_j=1$ for all $j$ and $\alpha=1$, so this is equivalent to the Lasso penalty. The objective function that we would like to minimize is given as $$-\frac{1}{N} l(\alpha,\beta)+\lambda\sum_{j=1}^p|b_j|.$$

  
## Random Forest
We implement the random forest method directly from the `caret` package in R for a comparison to the proportional odds model. We chose this type of model since it is a classic machine learning analogue for generalized linear models. We use 5-fold Cross Validation on the training data to build the model. Additionally, a tuning grid is used to tune the parameter `mtry`, which defines the number of variables randomly collected to be sampled at each split time. The tuning grid used explored `mtry` values $1$, $2$, and $3$. 

### Ordinal Forest
It should be noted however, that the typical random forest classifier does not account for the fact that the classes are ordinal (i.e. class 1 is "closer" to class 2 than class 3). Therefore, it is of interest to explore a machine learning algorithm that can account for this. The R package `ordinalForest` does just that. We use the model with default hyperparameters on our data and compare to the traditional random forest and the proportional odds model.

### Downsampling
Since the levels of severity are not balanced, we also down-sampled the less severe outcomes to see if the predicted model improves in terms of accuracy and/or computation burden. Since the minority class, severity-3, has 1081 incidents in the training set, we randomly sampled 1081 incidents from severity-1 incidents and 1081 incidents from severity-2 incidents, so that the three levels are balanced. Random Forest and Ordinal Forest were applied to this data. 

## Evaluation and Comparison of Models

In order to compare the performance of the different models built, we use the attributes from the observations in our test set to predict their severity classification for each model. Then we quantitatively compare which observations were correctly classified.

One way to do this is to measure the **accuracy** of the models on the test set. To do this, the fraction of the correctly classified observations is divided by the total number of observations in the test set. 

Another method to evaluate categorical models is to use **Cohen's Kappa**, which scales the accuracy to account for the predictions that are correct by guessing. We predict this will be more useful to us than the accuracy metric, since most of our data points come from Severity level 1 and accuracy could be very high simply by guessing all observations are in this class.

We also compare the computational time it takes to build each model, called the **time to train**. This is important when working with large datasets. 

And finally, for a more visual comparison, we can look at the **Confusion Matrix** for the test set. This will be a 3 x 3 matrix in our case, which tells us how many observations are predicted as class 1, 2, or 3 when their true classification is 1, 2, or 3.

# Results 

Note that not all of the code is run in this markdown document due to compiling time. If you’re interested in running the code for `stepAIC`, `ordinalNet`, computing correlation, and the forest-based methods, please see `var.selection.R`, `penalized.R`, `corr.R`, and `random_forest.R` respectively in the *scripts* folder of our [GitHub repository](https://github.com/kennyitang/BIOS735-Group3/tree/master/scripts). 

## Proportional Odds Model

After using `stepAIC` to select the optimal model from our full model of 12 predictors, we built the model using the functions from `group3project` and used the results to predict on our test set. Then, the model was compared to the penalized `ordinalNet`.

### Variable Selection via `stepAIC`

Using `stepAIC`, the optimal model was equivalent to the full model, with all of the 12 predictors kept in the model. This model will be used to build a model using the `group3project` R-package and to present our results of the proportional odds model. Note that here we are using the results from `polr` instead of making our own model compatible with `stepAIC`. But since the likelihoods are exactly the same and the difference in parameter estimates is in the magnitude of $10^{-4}$ or even smaller, we expect `stepAIC` will give us the same results if we input our model from `group3project`.

```{r, eval = FALSE}
mod <- polr(Severity_c ~ Source + Side + temperature_sc + humdity_sc + pressure_sc + 
              Visibility_sc + windspeed_sc + Crossing + Traffic_Signal +
              Sunrise_Sunset + weekday + interstate, data = trn_data, Hess=TRUE) 
step.model <- stepAIC(mod, direction = "both", trace = F)
summary(step.model)
```


### Examine association between covariates

Below are the plots showing the Pearson correlation between continuous covariates and Cramer's V for nominal covariates. Cramer's V is a measure of association between nominal categorical variables, giving a value between 0 and 1, based on the Pearson's chi-squared statistic. From both plots, the largest magnitude of the measure of association is about 0.4, and the majority of covariate pairs have weak association. We also examined the boxplots of each pair of continuous and nominal covariates, and found no obvious association among them. Therefore, our model with only the main effects should be adequate and do not suffer too much from collinearity. 

```{r fig.width=8,echo=FALSE}
img1 <-  grid::rasterGrob(as.raster(readPNG("corr.png")),
                            interpolate = FALSE)
img2 <-  grid::rasterGrob(as.raster(readPNG("cramer.png")),
                          interpolate = FALSE)
gbm::grid.arrange(img1, img2, ncol = 2)
```


### Proportional Odds via `group3project`

The following code was used to build the model.

```{r}
trn_data$Severity_c = factor(trn_data$Severity, levels = c(1,2,3), 
                             labels = c(1:length(unique(trn_data$Severity))), ordered = T)
trn_data$temperature_sc = scale(trn_data$`Temperature(F)`)
trn_data$humdity_sc = scale(trn_data$`Humidity(%)`)
trn_data$pressure_sc = scale(trn_data$`Pressure(in)`)
trn_data$windspeed_sc = scale(trn_data$`Wind_Speed(mph)`)
trn_data$Visibility_sc = scale(trn_data$`Visibility(mi)`)

start = Sys.time()
estimates = pom.est(Severity_c ~ Source + Side + temperature_sc + humdity_sc + pressure_sc + 
                     Visibility_sc + windspeed_sc + Crossing + Traffic_Signal +
                     Sunrise_Sunset + weekday + interstate, data = trn_data, SE = T, details = F)
end = Sys.time() 
print(end - start) 

g <- ggplot(estimates[3:15,], aes(x = reorder(as.factor(Variable), -abs(Estimates)), y = Estimates)) + 
  geom_bar(stat = "identity", color = "black", position = position_dodge(), fill = '#4472C4')+ 
  geom_errorbar(aes(ymin=Estimates-SE, ymax=Estimates+SE), width = .2,
                position = position_dodge(0.9)) + xlab("Predictors") + theme_bw()+
  theme(text = element_text(size=18),axis.text.x = element_text(size = 16,angle = 45, hjust = 1, vjust = 1))+
  scale_y_continuous(limits = c(-8,8)) + ggtitle("Proportional Odds Model Coefficient Estimates")
plot(g)

```

The coefficient estimates can be interpreted as the magnitude and direction of the predictor's influence on the probabilities of observing an accident with a certain level of severity, *conditional on* when accidents happened. It is worth pointing out that, the direction of the effect of a predictor $\hat{\beta_j}$ is unambiguously determined by the sign of the coefficient only for $P(y=1 | \mathbf{x}_i)$ and $P(y=3 | \mathbf{x}_i)$, the probabilities of the smallest and the highest response category<sup>[7]</sup>. The effect of a predictor on $P(y=2 | \mathbf{x}_i)$ can go either direction.

Specifically, our coefficient estimates suggest that, *given* that an accident happened, being on an interstate significantly increases the probability that this accident has a level-3 severity (on average from about 0.1% probability to roughly 10%), rather than a level-1 severity (from about 90% to a third of that). Having a traffic signal or pedestrian crossing nearby significantly decreases the probability that the accident has a level-3 severity (from about 0.1% to about half of that) while increasing the probability that the accident has a level-1 severity (by 5-10% on average).

The parametric proportional odds model approach allows us to identify influential predictors, as well as the direction and magnitude of their effect. However, We noticed that our intercept estimates were about 3.34 and 6.46, suggesting an extreme shift in the nonlinear CDF that left little probabilities of predicting a highest level of car accident severity. This signals a limitation of the parametric proportional odds model in comparison to the random forest approach. The code used to predict this model's performance on the test dataset can be seen at the bottom of `pom_bfgs_pred.R` in the *scripts* folder of our [GitHub repository](https://github.com/kennyitang/BIOS735-Group3/tree/master/scripts). 

### Comparison with the penalized proportional odds model

The code below builds the penalized `ordinalNet` model.

```{r, eval = FALSE}
# fit the full model with 12 predictors 
formula = Severity_c ~ Source + Side + temperature_sc + humdity_sc + pressure_sc + 
  Visibility_sc + windspeed_sc + Crossing + Traffic_Signal +
  Sunrise_Sunset + weekday + interstate

x = recode.factor(formula = formula, data = trn_data)
y = trn_data[, "Severity_c"]

set.seed(1)
time.start <- Sys.time()
fit <- ordinalNet(as.matrix(x), y, 
                   family="cumulative", link="logit",
                   parallelTerms=TRUE, nonparallelTerms=FALSE)
time.stop <- Sys.time()
time.stop - time.start

# predict with the test dataset 
tst_data2 <- tst_data %>% mutate_at(c("Temperature(F)",
                                      "Humidity(%)",
                                      "Pressure(in)",
                                      "Wind_Speed(mph)",
                                      "Visibility(mi)"), ~(scale(.) %>% as.vector))

colnames(tst_data2)[3:7] =c("temperature_sc", "humdity_sc", "pressure_sc", "windspeed_sc", "Visibility_sc")

testx = recode.factor(formula = formula, data = tst_data2)

predicted <- predict(fit, newx = as.matrix(testx), type="class")

# accuracy
calc_acc(actual = tst_data2$Severity_c, predicted = predicted)
# confusion matrix
confusionMatrix(as.factor(predicted), tst_data2$Severity_c)
```

Below is a plot comparing the coefficient estimates from the proportional odds model and the penalized model. We can see that the parameter estimates from ordinalNet have smaller magnitude in most cases, which is what we expected, but overall still largely consistent. The coefficients of variables `Source_MapQuest-Bing`, `Temperature`, and `Wind speed` were shrunk to zero. Notice that `Source_MapQuest-Bing` has a coefficient estimate around 0.3 in the proportional odds model, and the other two variables have very small effect sizes. Weather condition variables have small effect sizes in both model.

```{r fig.width=8,echo=FALSE}
grid.raster(readPNG("ordinal.png"))
```

Below is a table of the confusion matrices of the proportional odds model and the penalized model. We can see that both methods did not predict an accident as severity level 3, which may be due in part to the unbalanced number of severity levels in our training dataset. The decision boundaries created by the models may have completely masked severity class 3, making it impossible to predict an observation with this severity correctly. Adding interaction terms or polynomial terms in our predictors may be of interest to try and avoid the masking. These two models produced similar overall prediction accuracy, which will be compared to the machine learning methods below.   

```{r, echo = FALSE}
# create table  
down_df <- data.frame(Reference = c("1", "2", "3"), 
                       "Predict1" = c(33959, 813, 543), 
                       "Predict2" = c(2753, 5207, 114), 
                       "Predict3" = c(0,0,0), 
                       "Predict.1" = c(33951, 778, 551), 
                       "Predict.2" = c(2761, 5242, 106), 
                       "Predict.3" = c(0, 0, 0))
knitr::kable(down_df, align = 'cccc', caption = "Proportional Odds and Ordinal Net Confusion Matrices", 
             booktabs=T) %>% 
  add_header_above(c("", "Proportional Odds" = 3, "Ordinal Net" = 3)) %>%
kable_styling(latex_options = c("repeat_header", "scale_down"))
```



## Random Forest

#### RF Full Training Data

The code run to build the Random Forest on the training data with all predictors and test its predictive performance on the test set is given below.

```{r, eval = FALSE}
#5-fold cross-validation
cv_5 = trainControl(method = "cv", number = 5)

#Tuning parameter
rf_grid = expand.grid(mtry = 1:3)

#Train model 
set.seed(3)
start = Sys.time()
rf_mod_red = train(
  Severity_c ~ Source, Side + `Temperature(F)` + `Humidity(%)` + `Pressure(in)` + 
               `Visibility(mi)` + `Wind_Speed(mph)` + Crossing + Traffic_Signal +
               Sunrise_Sunset + weekday + interstate, 
  data = trn_data, 
  method = "rf",
  trControl = cv_5,
  tuneGrid = rf_grid
)
end = Sys.time()
print(end - start)

#Create plot of variable importance 
varImpPlot(rf_mod_red$finalModel)

#Obtain confusion matrix, accuracy, and kappa 
confusionMatrix(predict(rf_mod_red, newdata = tst_data), tst_data$Severity_c)
```

Below is a plot of variable importance for the random forest model. It is clear that the interstate predictor is the most important in predicting severity. The second most important is the sourceMapQuest variable, which is undesirable. We hoped that there would be no discrepancy between the severity scales of the different API Sources, but instead it appears that the Source is important in predicting severity, which is not a practical predictor to include in a model. Later, we will explore some Source-Specific models as potential remedies to overcome this so-called "batch-effect".

```{r fig.width=6,echo=FALSE}
grid.raster(readPNG("varimp_rf.png"))
```

#### OF Full Training Data 

The code run to build the Ordinal Forest on the training data with all predictors and test its predictive performance on the test set is given below.

```{r, eval = FALSE}
#Prepare data
datatrain = trn_data%>% dplyr::select(Source, Side, `Temperature(F)`, `Humidity(%)`, `Pressure(in)`,
                                `Visibility(mi)`, `Wind_Speed(mph)`, Crossing, Traffic_Signal,
                                 Sunrise_Sunset, weekday, interstate, Severity_c)
datatest = tst_data %>% dplyr::select(Source, Side, `Temperature(F)`, `Humidity(%)`, `Pressure(in)`,
                                `Visibility(mi)`, `Wind_Speed(mph)`, Crossing, Traffic_Signal,
                                Sunrise_Sunset, weekday, interstate, Severity_c)

#Train Model 
set.seed(13847)
start = Sys.time()
ordforest <- ordfor(depvar = "Severity_c", data = datatrain)
sort(ordforest$varimp, decreasing = TRUE)
end = Sys.time()
print(end - start)

#Predict 
preds <- predict(ordforest, newdata = datatest)

#Obtain confusion matrix, accuracy, and kappa 
confusionMatrix(preds$ypred, datatest$Severity_c)
```

Below are the confusion matrices for the Random Forest and Ordinal Forest predictions on the test set defined as the 2019 car accidents in North Carolina. While the matrices are similar, we see that the Ordinal Forest method correctly classifies more severity class 2 and 3 observations than Random Forest as seen along the diagonals of the matrices. Additionally and consequently, Ordinal Forest misclassifies less class 2 and 3 observations as class 1, as seen in the first column of each matrix.

```{r, echo = FALSE}
# create table  
down_df <- data.frame(Reference = c("1", "2", "3"), 
                       Predict1 = c(33838, 665, 522), 
                       Predict2 = c(2849, 5346, 94), 
                       Predict3 = c(25, 9, 41), 
                       Predict.1 = c(33706, 558, 458), 
                       Predict.2 = c(2963, 5446, 127), 
                       Predict.3 = c(43, 16, 72))
knitr::kable(down_df, align = 'cccc', caption = "RF and OF Confusion Matrices", 
             booktabs=T) %>% 
  add_header_above(c("", "Random Forest" = 3, "Ordinal Forest" = 3)) %>%
kable_styling(latex_options = c("repeat_header", "scale_down"))
```

#### Downsampling Code 

The following code creates a downsampled training data set from the original training data set, by taking a random sample, the size of the minority class, from the other two classes.

```{r, eval = FALSE}
set.seed(12984)
downsample.trn <- trn_data[c(which(trn_data$Severity_c == 3),
                             sample(which(trn_data$Severity_c == 1), 1081), 
                             sample(which(trn_data$Severity_c == 2), 1081)),]
```

The same code that was used to train Random Forest and Ordinal Forest on the full training data was repeated on the Downsampled data. Below are the confusion matrices for the Random Forest and Ordinal Forest predictions on the test set when the models were built from the down sampled training set. We don't see a big difference between the two methods, but compared to the full training set, we see more predictions for severity class 3, both correct and incorrect classifications.

```{r, echo = FALSE}
# create table  
down_df <- data.frame(Reference = c("1", "2", "3"), 
                       Predict1 = c(32478, 201, 20), 
                       Predict2 = c(2929, 5433, 79), 
                       Predict3 = c(1305, 386, 558), 
                       Predict.1 = c(32456, 201, 11), 
                       Predict.2 = c(2911, 5426, 79), 
                       Predict.3 = c(1345, 393, 567))
knitr::kable(down_df, align = 'cccc', caption = "Down Sampled Confusion Matrices", 
             booktabs=T) %>% 
  add_header_above(c("", "Random Forest" = 3, "Ordinal Forest" = 3)) %>%
kable_styling(latex_options = c("repeat_header", "scale_down"))
```

## Full Model Method Comparison 

The table below compares the 6 models built with all 12 predictors. We can see that the accuracy and Cohen's Kappa values are very comparable between the four models built with the entire Training Dataset. These metrics are slightly worse for the down sampled models. This is likely because of the immense loss of information when we apply a strict down sampling approach to our very imbalanced data. The most interesting result here is the difference in time to train between the models. 

```{r, echo = FALSE}
# create table
table_df <- data.frame(Method = c("Proportional Odds", "OrdinalNet", "Random Forest", "Ordinal Forest", "RF Down Sample", "OF Down Sample"), 
                       Training_Set_Size = c(rep(75887,4), rep(3243, 2)), 
                       Accuracy = c(0.903, .903, .904, .904, 0.887, 0.886), 
                       Kappa = c(0.659, 0.662, 0.6696, 0.675, 0.661, 0.660), 
                       Time = c("1.7 minutes", "5.4  minutes", "11.5 minutes", "1.5 hours", "24 seconds", "7 minutes"))
knitr::kable(table_df, align = 'ccccc', caption = "Model Performance",  booktabs = TRUE) %>%
kable_styling(latex_options = c("repeat_header", "scale_down"))
```

## Source Specific Modeling 
Due to the results from the full training dataset, it is clear that the Source Predictor is important in predicting the severity of car accidents on traffic delays. It is not practical to use a prediction model with a variable such as API data source as an important predictor. The different sources should ideally fit seamlessly together, but instead we have heterogeneity among the MapQuest and Bing derived data. Given more time, it might be of interest to reach out to these API sources to gather more information about their respective definitions of severity so we could try to create a more harmonious output variable. In the meantime, we build three new Random Forest models to investigate this issue further. 

First, we build another model on the full training dataset, but this time removing Source from the set of predictors. 

```{r, eval = FALSE}
#5-fold cross-validation
cv_5 = trainControl(method = "cv", number = 5)

#Tuning parameter
rf_grid = expand.grid(mtry = 1:3)

#Train model 
set.seed(3)
start = Sys.time()
rf_mod_red = train(
  Severity_c ~ Side + `Temperature(F)` + `Humidity(%)` + `Pressure(in)` + 
               `Visibility(mi)` + `Wind_Speed(mph)` + Crossing + Traffic_Signal +
               Sunrise_Sunset + weekday + interstate, 
  data = trn_data, 
  method = "rf",
  trControl = cv_5,
  tuneGrid = rf_grid
)
end = Sys.time()
print(end - start)

#Create plot of variable importance 
varImpPlot(rf_mod_red$finalModel)

#Obtain confusion matrix, accuracy, and kappa 
confusionMatrix(predict(rf_mod_red, newdata = tst_data), tst_data$Severity_c)
```

Below is a plot of variable importance for the random forest model. It is clear that the 
interstate predictor is still the most important in predicting severity, as was the case when the Source predictor was included in the model. 

```{r fig.width=6,echo=FALSE}
grid.raster(readPNG("varimp_nosource.png"))
```

Below are the confusion matrices for the Random Forest predictions on the test set with and without the Source predictor. The 'with' confusion matrix is the same one seen above, placed here again for comparison. We see that no class 3 observations are classified correctly, which is an unfortunate consequence of removing this important predictor.

```{r, echo = FALSE}
# create table  
down_df <- data.frame(Reference = c("1", "2", "3"), 
                       Predict1 = c(33838, 665, 522), 
                       Predict2 = c(2849, 5346, 94), 
                       Predict3 = c(25, 9, 41), 
                       Predict.1 = c(34300, 2364, 530), 
                       Predict.2 = c(2247, 3655, 127), 
                       Predict.3 = c(165, 1, 0))
knitr::kable(down_df, align = 'cccc', caption = "RF Confusion Matrices with and without Source Predictor", 
             booktabs=T) %>% 
  add_header_above(c("", "With Source" = 3, "Without Source" = 3)) %>%
kable_styling(latex_options = c("repeat_header", "scale_down"))
```

Second, we subset the processed training and testing data into two groups each: one with `Source = MapQuest` and one with `Source = Bing`. The few observations that were obtained from both APIs were dropped. Additionally, since only 8 observations from MapQuest had severity 3, those observations were dropped and the MapQuest model reduced to a two level outcome model. The breakdown of accidents per severity class in both of these training data subsets are output from the code below. We can see in both cases, the outcome remains unbalanced among the classes.

```{r}
bing.trn <- trn_data[c(which(trn_data$Source == "Bing")), ]
bing.test <- tst_data %>% dplyr::filter(Source == "Bing")
table(bing.trn$Severity_c)

mq.trn <- trn_data %>% dplyr::filter(Source == "MapQuest" & !(Severity_c == "3"))
mq.test <- tst_data %>% dplyr::filter(Source == "MapQuest" & !(Severity_c == "3"))
mq.test$Severity_c <- droplevels(mq.test$Severity_c)
mq.trn$Severity_c <- droplevels(mq.trn$Severity_c)
table(mq.trn$Severity_c)
```

The same code that was used to train Random Forest on the full training data was repeated on the source specific data. Below are the plots of variable importance for the random forest models (Bing on the left and MapQuest on the right). It is clear that the 
predictor Interstate is still the most important in predicting severity for the MapQuest specific model only. For the Bing model, Interstate is of middle importance compared to the rest of the predictors, and Temperature is the most important. 

```{r, echo=FALSE}
img1 <-  grid::rasterGrob(as.raster(readPNG("varimp_bing.png")),
                            interpolate = FALSE)
img2 <-  grid::rasterGrob(as.raster(readPNG("varimp_mq.png")),
                          interpolate = FALSE)
gbm::grid.arrange(img1, img2, ncol = 2)
```

Below are the confusion matrices for the Bing and MapQuest model predictions on the Bing and MapQuest specific test sets. While the MapQuest model appears to correctly predict most observations, the Bing model predicts a very small percentage of class 2 and 3 correctly, placing most of its predictions in class 1.

```{r, echo = FALSE}
# create table  
down_df <- data.frame(Reference = c("1", "2", "3"), 
                       Predict1 = c(1314, 364, 487), 
                       Predict2 = c(44, 20, 26), 
                       Predict3 = c(30, 13, 63), 
                       Predict.1 = c(32460, 267, "."), 
                       Predict.2 = c(2820, 5329, "."))
knitr::kable(down_df, align = 'cccc', caption = "Source Specific Confusion Matrices", 
             booktabs=T) %>% 
  add_header_above(c("", "Bing" = 3, "MapQuest" = 2)) %>%
kable_styling(latex_options = c("repeat_header", "scale_down"))
```

The table below compares the 3 models built exploring the Source Predictor. We can see that removing the Source predictor for the full training data does not negatively impact the accuracy and Kappa values too much. For the MapQuest specific model, the accuracy and Kappa values are the highest seen of any of the models thus far. For the Bing specific model, it's exactly the opposite, the lowest seen so far.  

```{r, echo = FALSE}
# create table  
table_df <- data.frame(Method = c("Full Data No Source", "MapQuest Data", "Bing Data"), 
                       Training_Set_Size = c(75887, 59538, 15518), 
                       Accuracy = c(0.875, .924, .592), 
                       Kappa = c(0.510, 0.732, 0.0795), 
                       Time = c("12 minutes", "8  minutes", "2 minutes"))
knitr::kable(table_df, align = 'ccccc', caption = "Performance for Source Specific Random Forest Models", booktabs = TRUE) %>%
kable_styling(latex_options = c("repeat_header", "scale_down"))
```

# Discussion 

Although our accuracy values appear very high at first, we realize that API Source the data came from can greatly affect the results. This "batch effect" is undesirable when building a prediction model that should integrate these sources seamlessly. Aside from this issue, the most important variable in predicting car accident related traffic severity appears to be Interstate. In terms of our models, the Proportional Odds Model, Ordinal Net, Random Forest, and Ordinal Forest seem to perform similarly (in terms of accuracy/Kappa) on our test dataset with an outcome factor with 3 levels, although the time to train varied widely. 

It is also clear that summary statistics such as accuracy/Cohen's Kappa do not always tell the whole story. Although they had similar accuracies, the confusion matrices showed us that the likelihood-based models on the full training data had no predictions for severity class 3, whereas the forest-based machine learning models did. This could be because of the Random Forest's ability to account for more complicated interactions between the variables, whereas our proportional odds model had no interaction terms included. 

When looking into Source specific models, we also discovered that the MapQuest data alone is more successful in predicting severity of other MapQuest based accidents than Bing data or the combined data.

A lot more data or different types of data may be needed to fully answer the questions this project aimed at answering, but this novel large scale and these types of analyses are a step in the right direction.

## Limitations 

As mentioned earlier, our outcome variable severity was defined very vaguely from the two API sources. It is not clear exactly how these classes are derived and whether the "distance" between class 1 and 2 is the same as between class 2 and 3, within and across the two sources. This limitation contributes to our inability to truly understand the API Source effect on the outcome.

Another limitation is the imbalance of data we have in the training set, with most of the observations in severity class 1. Although we tried down-sampling, this resulted in an immense loss of information, using about 3,000 of our 75,000 training accidents. 

There is also lack of information about factors that we believe may be important such as speed limit and type of vehicle(s) involved in the accident. If these were included, we may be able to build a model that captures the full picture. 

## Future Directions 

We assumed that the proportional odds assumption holds and also assumed there were no longitudinal effects in our data. Testing both of these assumptions would be needed moving forward to ensure our methods are sound. Additionally, adding a penalty feature to our own proportional odds model functions would be a next step, to see how that could potentially improve the likelihood-based models, and potentially allow for interactions or more variables from other data sets.

It would also be of interest to aggregate our outcome further into two severity classes by combining classes 2 and 3 assuming the "distance" between class 2 and 3 is close, in order to compare models of this type to those we have already built. It would also be useful to try and account for the imbalanced data in other ways, potentially by using a weighted Random Forest, or even by borrowing data from other states in the minority classes.

In terms of the data, we would like to expand into more states and also incorporate local law enforcement data sets that contain information on the severity of car accidents in terms of injuries, fatalities, etc. One example source of this data is [Town of Cary Crash Data](https://data.world/townofcary/cpd-crash-incidents).

# Data Source 
The Car Accidents data set for North Carolina and the derived training and test data sets are provided in our project repository. Due to the size, we do not include the original nation-wide car accidents data set in our repository, but interested reader can find the dataset and related information on [Kaggle](https://www.kaggle.com/sobhanmoosavi/us-accidents) and the author's [Website](https://smoosavi.org/datasets/us_accidents).  
  
  
# References 
[1] Levy, J.I., Buonocore, J.J. & von Stackelberg, K. Evaluation of the public health impacts of traffic congestion: a health risk assessment. Environ Health 9, 65 (2010). https://doi.org/10.1186/1476-069X-9-65

[2] Retallack, A. E., & Ostendorf, B. (2019). Current Understanding of the Effects of Congestion on Traffic Accidents. International journal of environmental research and public health, 16(18), 3400. https://doi.org/10.3390/ijerph16183400

[3] Moosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, and Rajiv
      Ramnath. “A Countrywide Traffic Accident Dataset.”, 2019.
  
[4] Moosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, Radu
      Teodorescu, and Rajiv Ramnath. "Accident Risk Prediction based on Heterogeneous
      Sparse Data: New Dataset and Insights." In proceedings of the 27th ACM SIGSPATIAL
      International Conference on Advances in Geographic Information Systems, ACM, 2019.

[5] MapQuest documentation on traffic incident data: https://developer.mapquest.com/documentation/traffic-api/incidents/get/#response_field-severity

[6] Bing documentation on traffic incident data: https://docs.microsoft.com/en-us/bingmaps/rest-services/traffic/traffic-incident-data

[7] Wooldridge, Jeffrey. "M.(2002) Econometric Analysis of Cross Section and Panel Data." The MIT Press 0 5.1 (1995): 5.

[8] Pratt, J. Concavity of the Log Likelihood. Journal of the American Statistical Association, 76(373), 103-106 (1981). doi:10.2307/2287052

