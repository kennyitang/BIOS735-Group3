---
title: 'BIOS 735 Final Project: US Car Accidents and Traffic Severity'
author: "Marissa Ashner, Yen Chang, Marco Chen, Yi Tang Chen, Weifang Liu, Joyce Yan"
date: "4/27/2020"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_float: true
    theme: journal
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(png)
library(grid)
library(kableExtra)
```

# Introduction 

Sitting in traffic is not ideal for anyone, and it is especially frustrating when the traffic is caused by an accident. It is an unexpected delay, and many times it is completely uncertain how long the delay will last.  For a country where the majority of the population relies so heavily on automobiles for daily transportation, traffic delays may result in a multitude of disruptions in many other aspects of life as well.  

Not only is sitting traffic frustrating, but the resulting air pollution and long-term exposure to vehicle emissions may also be associated with future health and respiratory issues.<sup>[5]</sup>  Moreover, traffic accidents pose a big problem for public safety. In many studies, traffic congestion has been considered a potentially important factor impacting the occurrence of accidents,<sup>[6]</sup> suggesting that the severity of a traffic delay caused by one accident may influence the likelihood of a secondary accident.

Traffic accident data can therefore be an invaluable resource, as it can provide information that can potentially aid in predicting conditions where traffic accidents are prone to happen, so that steps can be taken to prevent the accident. Further, this data could also be useful in predicting how severely an accident will delay the drivers following the accident, so that steps can be taken to reduce this delay as needed. 

Several computer scientists from the Ohio State University present exactly this: a dataset of traffic accidents from the contiguous United States, covering 2.95 million accidents with data collected between February 2016 and December 2019. As of March 2019, of the 49 states included, North Carolina had the fourth highest number of accidents in the dataset at 109,000. <sup>[1]</sup> Because of the wide range of attributes collected with each accident in the dataset, this dataset has the potential to provide the information needed to start making decisions about how to mitigate traffic accidents and the delays associated.

# Project Aim 

The aim of this project is to use Car Accident Data from North Carolina in 2016-2018 to determine which factors have the most influence on the severity of car accidents in the state. We take two different approaches to construct models to predict the severity of car accidents, one through a proportional odds model and the other through machine learning. We then use the 2019 data to evaluate and compare the performance of our prediction models.

# R-Package

Our R package `group3project` contains the functions needed to fit a proportional odds model and random forest as well as the datasets we used to do so for this report. You can find this package on [GitHub](https://github.com/kennyitang/BIOS735-Group3/tree/master/group3project).

*Note: edit this as needed as we fill in the R package

* Functions for Proportional Odds Model
    + loglik.pom: computes log likelihood of a proportional odds model
    + gradient.pom:      <--which one to use?
    + gradient.pom2:
    

* Functions for Random Forest
    + calc_acc: calculates accuracy of classification

* Datasets 
    + Testing Data (tst_data)
    + Training Data (trn_data)
    + Full NC Data (nc_data)

```{r, message = FALSE}
## load our package and datasets here
devtools::load_all("../group3project")
data("trn_data")
data("tst_data")
```

# Data
The original dataset had 49 total attributes, including attributes related to traffic, location, weather, points of interest, and time of day.  Pre-processing the data set was necessary to allow us to focus on the variables and observations of real relevance to our aim. Although we started out with 48 dependent variables and one outcome variable, there were several reasons for removing most of the attributes, including:  

1. The attribute had at least 60% missingness.
    + Wind Chill and Precipitation
2. The attribute had very low or no variability across the observations.  
    + after aggregating the County variable into urban and rural factor levels, it was apparent that about 95% of the accident data was derived from urban areas, therefore variables such as County, Zip-Code, and City were not of use 
    + most of the points of interest variables except Crossing and Traffic Signal
3. The attribute was relevant for spatial purposes (i.e. latitude and longitude), which we do not aim to study at this time.

Therefore this information was not useful for prediction. 

In terms of traffic incidents, rows missing all weather variables were removed. After pre-processing, the analysis dataset contains 119,279 traffic accidents from North Carolina between the years 2016 and 2019 with 12 attributes and one outcome variable. Additionally, the analysis data were split up into training data (pre-2019) and testing data (2019) as mentioned previously. The final training dataset has 75,887 observations and the testing has 43,392 observations.

If you’re interested in looking at the code for the Data Processing and Exploratory Analysis, please see **data_prep.R** and **exploratory_analysis.R** in the scripts folder of our [GitHub repository](https://github.com/kennyitang/BIOS735-Group3/tree/master/scripts). 
  
The variables selected for analysis, and some accompanying exploratory plots are as follows: 

### Outcome:
1. Severity:  severity of the accident in terms of the impact on traffic, which is an integer from 1 to 3 with increasing impact. This is an ordinal variable and the outcome of interest.  

Originally this variable had four categories (1-4), but only 24 observations had severity of 1, so we combined severities 1 and 2 and readjusted the scale. Even after this aggregation, our dataset is still very unbalanced. As seen in the code output below, severity class 1 has many more accidents in the training data than the more severe classes.

```{r}
trn_data$Severity_c = as.factor(trn_data$Severity)
tst_data$Severity_c = as.factor(tst_data$Severity)
table(trn_data$Severity_c)
```

It should be noted that the data set sources are rather vague on the definition of this outcome variable. Length of delay caused by the accident is the major determinant, and we assume that scope, such as how many lane closures there were, may also be taken into account when determining the severity. 

### Predictors:

1. Source: categorical, API that reported accident (MapQuest, Bing, MapQuest-Bing)  

**Location/Time** 

2. Side: categorical, side of street (Left/Right)  
3. Sunrise_Sunset: categorical (Day/Night)
4. Weekday: TRUE/FALSE, if accident started on a weekday 
5. Interstate: TRUE/FALSE, if accident happened on an interstate
  
**Weather**  

6. Temperature: numeric, degrees Fahrenheit    
7. Humidity: numeric, percentage  
8. Pressure: numeric air pressure, in inches  
9. Visibility: numeric, in miles  
10. Wind Speed: numeric, in mph 

```{r fig.width=8,echo=FALSE}
grid.raster(readPNG("weather_boxplots.png"))
```
  
**Points of Interest**  

11. Crossing: TRUE/FALSE, presence of a crossing nearby for pedestrians, cyclists, etc. 
12. Traffic_Signal: TRUE/FALSE, presence of a traffic signal nearby on an intersection 

```{r,echo=FALSE}
img1 <-  grid::rasterGrob(as.raster(readPNG("crossing.png")),
                            interpolate = FALSE)
img2 <-  grid::rasterGrob(as.raster(readPNG("Traffic_Signal.png")),
                          interpolate = FALSE)
gbm::grid.arrange(img1, img2, ncol = 2)
```

# Methods 
We apply a proportional odds model with severity of an accident as the outcome, and predictors listed in the previous section as covariates. Based on paramter estimates of this model <---potentially with penalization--->, we identify variables that are most associated with severity. In parallel, starting with the same set of variables, we use random forest to find the predictors and build a model that yields the best classification.  


## Proportional Odds Model  
For a single severity outcome $Y_i$ with covariates $X_i$, the usual proportional odds model suggests that 
\[
  {\displaystyle
    \begin{aligned}
      logit(P(Y_i\leq k))=log(\frac{P(Y_i\leq k)}{1-P(Y_i\leq k)}) = \alpha_k - x_i^T\beta\\
      &\qquad\qquad\qquad &\qquad\qquad\qquad &\qquad\qquad\qquad &\qquad\qquad\qquad &\qquad\qquad\qquad &\qquad\qquad\qquad &\qquad\qquad
    \end{aligned}
  }
\], where \(\beta = (\beta_1,...,\beta_p)^T, x_i = (x_{i1},..., x_{ip})^T,\) and \(k= 1, 2\).

<br>

Equivalently, if we define \(\phi(t) = \frac{1}{1+exp(-t)}\), then 
\[  
{\displaystyle
    \begin{aligned}
P(Y_i\leq k) = \frac{1}{1 + exp(x_i^T\beta-\alpha_k)} = \phi(\alpha_k-x_i^T\beta).&\\
                                   & \qquad\qquad\qquad &\qquad\qquad\qquad &\qquad\qquad\qquad &\qquad\qquad\qquad &\qquad\qquad\ 
    \end{aligned}
  }  
\]

If we define $\alpha = (\alpha_1,\alpha_2)^T$, then log likelihood function for n accidents is therefore given by 
\[
  {\displaystyle
    \begin{aligned}
l(\alpha, \beta)&= \sum_{i=1}^n\sum_{k=1}^3I(Y_i=k)log(P(Y_i=k))\\
                &=\sum_{i=1}^n \{I(Y_i=1)log(P(Yi\le1))+ I(Y_i=2)[log(P(Yi\le2)-P(Y_i\le1)] + I(Y_i=3)[log(1-P(Y_i\le2)]\}\\
                &=\sum_{i=1}^n\{I(Y_i=1)log(\phi(\alpha1-x_i^T\beta))+ I(Y_i=2)log(\phi(\alpha2-x_i^T\beta)-\phi(\alpha1-x_i^T\beta))+  I(Y_i=3)log(1-\phi(\alpha2-x_i^T\beta))\}
    \end{aligned}
  }
\].  

The gradient is 
\[
  {\displaystyle
    \begin{aligned}
(\frac{\partial l(\alpha, \beta)}{\partial \beta_1}, ..., \frac{\partial l(\alpha, \beta)}{\partial \beta_p}, \frac{\partial l(\alpha, \beta)}{\partial \alpha_1}, \frac{\partial l(\alpha, \beta)}{\partial \alpha_2})^T&\\
                                   & \qquad\qquad\qquad &\qquad\qquad\qquad &\qquad\qquad\qquad & 
    \end{aligned}
  }
\]

, with the individual elements given below.

<br>

\[
  {\displaystyle
    \begin{aligned}
        \frac{\partial l(\alpha, \beta)}{\partial \beta_j}
          &= \sum_{i=1}^n\left(-I(Y_i=1)\frac{exp(x_i^T\beta-\alpha_1)}{1+exp(x_i^T\beta-\alpha_1)}x_{ij}
            + I(Y_i=2)\frac{\frac{exp(x_i^T\beta-\alpha_1)}{[1+exp(x_i^T\beta-\alpha_1)]^2}-
                \frac{exp(x_i^T\beta-\alpha_2)}{[1+exp(x_i^T\beta-\alpha_2)]^2}}{\phi(\alpha_2-x_i^T\beta)-\phi(\alpha_1-x_i^T\beta)}x_{ij} 
            + I(Y_i=3)\frac{x_{ij}}{1+exp(x_i^T\beta-\alpha_2)}\right)
          \\&=\sum_{i=1}^n\left(-I(Y_i=1)(1-\phi(\alpha_1-x_i^T\beta)) 
            + I(Y_i=2)\frac{\phi(\alpha_1-x_i^T\beta)[1-\phi(\alpha_1-x_i^T\beta)]
              -\phi(\alpha_2-x_i^T\beta)[1-\phi(\alpha_2-x_i^T\beta)]}{\phi(\alpha_2-x_i^T\beta)-\phi(\alpha_1-x_i^T\beta)} 
            +I(Y_i=3)\phi(\alpha_2-x_i^T\beta)\right)x_{ij}\\
        \\
        \frac{\partial l(\alpha, \beta)}{\partial \alpha_1} 
        &= \sum_{i=1}^n \left(I(Y_i=1)\frac{exp(x_i^T\beta-\alpha_1)}{1+exp(x_i^T\beta-\alpha_1)}
          -I(Y_i=2)\frac{\frac{exp(x_i^T\beta-\alpha_1)}{[1+exp(x_i^T\beta-\alpha_1)]^2}}{\phi(\alpha_2-x_i^T\beta)-\phi(\alpha_1-x_i^T\beta)} \right)\\
        &=\sum_{i=1}^n\left( I(Y_i=1)[1-\phi(\alpha_1-x_i^T\beta)]
          - I(Y_i=2)\frac{\phi(\alpha_1-x_i^T\beta)[1-\phi(\alpha_1-x_i^T\beta)]}{\phi(\alpha_2-x_i^T\beta)-\phi(\alpha_1-x_i^T\beta)} \right)\\
        \\
     \end{aligned}
  }
\]

\[
  {\displaystyle
    \begin{aligned}
        \frac{\partial l(\alpha, \beta)}{\partial \alpha_2} &= \sum_{i=1}^n \left(
            I(Y_i=2)\frac{\frac{exp(x_i^T\beta-\alpha_2)}{[1+exp(x_i^T\beta-\alpha_2)]^2}}{\phi(\alpha_2-x_i^T\beta)-\phi(\alpha_1-x_i^T\beta)}
            -I(Y_i=3)\frac{\frac{exp(x_i^T\beta-\alpha_2)}{[1+exp(x_i^T\beta-\alpha_2)]^2}}{1-\phi(\alpha_2-x_i^T\beta)} \right)
            \\                       
            &=\sum_{i=1}^n\left( I(Y_i=2)\frac{\phi(\alpha_2-x_i^T\beta)[1-\phi(\alpha_2-x_i^T\beta)]}{\phi(\alpha_2-x_i^T\beta)-\phi(\alpha_1-x_i^T\beta)}
            - I(Y_i=3)\frac{\phi(\alpha_2-x_i^T\beta)[1-\phi(\alpha_2-x_i^T\beta)]}{1-\phi(\alpha_2-x_i^T\beta)} \right)      \qquad\qquad\qquad\qquad
  \end{aligned}
  }
\]   

### Proportional Odds Model with Penalty  
Although we only listed 13 predictors of interest, the Weather_Condition variable contains 53 levels alone, which can be problematic to fit especially if interactions are of interest. Meanwhile, aggregating the levels can be very subjective. To solve this problem, we integrate a <---insert the kind of penalty---> penalty term into the ordinary proportional odds model.

<---formulas for penalty and the likelihood function and gradient---->


### Variable Selection

  
## Random Forest
We implement the random forest method directly from the `caret` package in R for a comparison to the proportional odds model.  We use 5-fold Cross Validation on the training data to build the model. Additionally, a tuning grid is used to tune the parameter `mtry`, which defines the number of variables randomly collected to be sampled at each split time. The tuning grid used explored `mtry` values $1$, $2$, and $3$. 

### Ordinal Forest
It should be noted however, that the typical random forest classifier does not account for the fact that the classes are ordinal (i.e. class 1 is "closer" to class 2 than class 3). Therefore, it is of interest to explore a machine learning algorithm that can account for this. The R package `ordinalForest` does just that. We use the model with default hyperparameters on our data and compare to the traditional random forest and the proportional odds model.

### Downsampling
Since the levels of severity are not balanced, we also down-sampled the less severe outcomes to see if the predicted model improves in terms of accuracy and/or computation burden. Since the minority class, severity-3, has 1081 incidents in the training set, we randomly sampled 1081 incidents from severity-1 incidents and 1081 incidents from severity-2 incidents, so that the three levels are balanced. Random Forest and Ordinal Forest were applied to this data. 

## Evaluation and Comparison of Models

In order to compare the performance of the different models built, we will use the attributes from the observations in our test set to predict their severity classification for each model. Then we can quantitatively compare which observations were correctly classified.

One way to do this is to measure the **accuracy** of the models on the test set. To do this, the fraction of the correctly classified observations is divided by the total number of observations in the test set. 

Another method to evaluate categorical models is to use **Cohen's Kappa**, which scales the accuracy to account for the predictions that are correct by guessing. We predict this will be more useful to us than the accuracy metric, since most of our data points come from Severity level 1 and accuracy could be very high simply by guessing all observations are in this class.

We can also compare the computational time it takes to build each model, called the **time to train**. This is important when working with large datasets. 

And finally, for a more visual comparison, we can look at the **Confusion Matrix** for the test set. This will be a 3 x 3 matrix in our case which tells us how many observations were predicted as class 1, 2, or 3 when their true classification is 1, 2, or 3.

# Results 

Note that not all of the code is run in this markdown document due to compiling time. If you’re interested in running the code for the forest-based methods, please see **random_forest.R** in the scripts folder of our [GitHub repository](https://github.com/kennyitang/BIOS735-Group3/tree/master/scripts). 

## Proportional Odds Model
```{r}


```

```{r, echo = FALSE}
# create table  
down_df <- data.frame(Reference = c("1", "2", "3"), 
                       Predict1 = c(33959, 813, 543), 
                       Predict2 = c(2753, 5207, 114), 
                       Predict3 = c(0,0,0), 
                       Predict.1 = c(33906, 753, 540), 
                       Predict.2 = c(2806, 5267, 117), 
                       Predict.3 = c(0, 0, 0))
knitr::kable(down_df, align = 'cccc', caption = "Proportional Odds and Ordinal Net Confusion Matrices", 
             booktabs=T) %>% 
  add_header_above(c("", "Proportional Odds" = 3, "Ordinal Net" = 3)) %>%
kable_styling(latex_options = c("repeat_header", "scale_down"))
```

## Random Forest

#### RF Full Training Data

```{r, eval = FALSE}
#5-fold cross-validation
cv_5 = trainControl(method = "cv", number = 5)

#Tuning parameter
rf_grid = expand.grid(mtry = 1:3)

#Train model 
set.seed(3)
start = Sys.time()
rf_mod_red = train(
  Severity_c ~ Source, Side + `Temperature(F)` + `Humidity(%)` + `Pressure(in)` + 
               `Visibility(mi)` + `Wind_Speed(mph)` + Crossing + Traffic_Signal +
               Sunrise_Sunset + weekday + interstate, 
  data = trn_data, 
  method = "rf",
  trControl = cv_5,
  tuneGrid = rf_grid
)
end = Sys.time()
print(end - start)

#Create plot of variable importance 
varImpPlot(rf_mod_red$finalModel)

#Obtain confusion matrix, accuracy, and kappa 
confusionMatrix(predict(rf_mod_red, newdata = tst_data), tst_data$Severity_c)
```

```{r fig.width=6,echo=FALSE}
grid.raster(readPNG("varimp_rf.png"))
```

#### OF Full Training Data 

```{r, eval = FALSE}
#Prepare data
datatrain = trn_data%>% dplyr::select(Source, Side, `Temperature(F)`, `Humidity(%)`, `Pressure(in)`,
                                `Visibility(mi)`, `Wind_Speed(mph)`, Crossing, Traffic_Signal,
                                 Sunrise_Sunset, weekday, interstate, Severity_c)
datatest = tst_data %>% dplyr::select(Source, Side, `Temperature(F)`, `Humidity(%)`, `Pressure(in)`,
                                `Visibility(mi)`, `Wind_Speed(mph)`, Crossing, Traffic_Signal,
                                Sunrise_Sunset, weekday, interstate, Severity_c)

#Train Model 
set.seed(13847)
start = Sys.time()
ordforest <- ordfor(depvar = "Severity_c", data = datatrain)
sort(ordforest$varimp, decreasing = TRUE)
end = Sys.time()
print(end - start)

#Predict 
preds <- predict(ordforest, newdata = datatest)

#Obtain confusion matrix, accuracy, and kappa 
confusionMatrix(preds$ypred, datatest$Severity_c)
```

```{r, echo = FALSE}
# create table  
down_df <- data.frame(Reference = c("1", "2", "3"), 
                       Predict1 = c(33838, 665, 522), 
                       Predict2 = c(2849, 5346, 94), 
                       Predict3 = c(25, 9, 41), 
                       Predict.1 = c(33706, 558, 458), 
                       Predict.2 = c(2963, 5446, 127), 
                       Predict.3 = c(43, 16, 72))
knitr::kable(down_df, align = 'cccc', caption = "RF and OF Confusion Matrices", 
             booktabs=T) %>% 
  add_header_above(c("", "Random Forest" = 3, "Ordinal Forest" = 3)) %>%
kable_styling(latex_options = c("repeat_header", "scale_down"))
```

#### Downsampling Code 

The following code creates a downsampled training data set from the original training data set, by taking a random sample, the size of the minority class, from the other two classes.

```{r, eval = FALSE}
set.seed(12984)
downsample.trn <- trn_data[c(which(trn_data$Severity_c == 3),
                             sample(which(trn_data$Severity_c == 1), 1081), 
                             sample(which(trn_data$Severity_c == 2), 1081)),]
```

The same code that was used to train Random Forest and Ordinal Forest on the full training data was repeated on the Downsampled data. 

```{r, echo = FALSE}
# create table  
down_df <- data.frame(Reference = c("1", "2", "3"), 
                       Predict1 = c(32478, 201, 20), 
                       Predict2 = c(2929, 5433, 79), 
                       Predict3 = c(1305, 386, 558), 
                       Predict.1 = c(32456, 201, 11), 
                       Predict.2 = c(2911, 5426, 79), 
                       Predict.3 = c(1345, 393, 567))
knitr::kable(down_df, align = 'cccc', caption = "Down Sampled Confusion Matrices", 
             booktabs=T) %>% 
  add_header_above(c("", "Random Forest" = 3, "Ordinal Forest" = 3)) %>%
kable_styling(latex_options = c("repeat_header", "scale_down"))
```

## Full Model Method Comparison 

The table below compares the 6 models built with all 12 predictors. We can see that the accuracy and Cohen's Kappa values are very comparable between the four models built with the entire Training Dataset. These metrics are slightly worse for the down sampled models. This is likely because of the immense loss of information when we apply a strict down sampling approach to our very imbalanced data. The most interesting result here is the difference in time to train between the models. 

```{r, echo = FALSE}
# create table  
table_df <- data.frame(Method = c("Proportional Odds", "OrdinalNet", "Random Forest", "Ordinal Forest", "RF Down Sample", "OF Down Sample"), 
                       Training_Set_Size = c(rep(75887,4), rep(3243, 2)), 
                       Accuracy = c(0.903, .903, .904, .904, 0.887, 0.886), 
                       Kappa = c(0.659, 0.664, 0.6696, 0.675, 0.661, 0.660), 
                       Time = c("1.7 minutes", "5.4  minutes", "11.5 minutes", "1.5 hours", "24 seconds", "7 minutes"))
knitr::kable(table_df, align = 'ccccc', caption = "Model Performance", booktabs = TRUE) %>%
kable_styling(latex_options = c("repeat_header", "scale_down"))
```

## Source Specific Modeling 
Due to the results from the full dataset, it is clear that the `Source` Predictor is important in predicting the severity of car accidents on traffic delays in our dataset. It is not practical to use a prediction model with a variable such as API data source as an important predictor in the model. The different sources should ideally fit seamlessly together, but instead we have heterogeneity among the MapQuest and Bing derived data. Given more time, it might be of interest to reach out to these API sources to gather more information about their respective definitions of severity so we could try to create a more harmonious output. In the meantime, we build three new Random Forest models to investigate this issue further. 

First, we build a model on the full training dataset, but removing Source from the set of predictors. 

```{r, eval = FALSE}
#5-fold cross-validation
cv_5 = trainControl(method = "cv", number = 5)

#Tuning parameter
rf_grid = expand.grid(mtry = 1:3)

#Train model 
set.seed(3)
start = Sys.time()
rf_mod_red = train(
  Severity_c ~ Side + `Temperature(F)` + `Humidity(%)` + `Pressure(in)` + 
               `Visibility(mi)` + `Wind_Speed(mph)` + Crossing + Traffic_Signal +
               Sunrise_Sunset + weekday + interstate, 
  data = trn_data, 
  method = "rf",
  trControl = cv_5,
  tuneGrid = rf_grid
)
end = Sys.time()
print(end - start)

#Create plot of variable importance 
varImpPlot(rf_mod_red$finalModel)

#Obtain confusion matrix, accuracy, and kappa 
confusionMatrix(predict(rf_mod_red, newdata = tst_data), tst_data$Severity_c)
```

```{r fig.width=6,echo=FALSE}
grid.raster(readPNG("varimp_nosource.png"))
```

```{r, echo = FALSE}
# create table  
down_df <- data.frame(Reference = c("1", "2", "3"), 
                       Predict1 = c(33838, 665, 522), 
                       Predict2 = c(2849, 5346, 94), 
                       Predict3 = c(25, 9, 41), 
                       Predict.1 = c(34300, 2364, 530), 
                       Predict.2 = c(2247, 3655, 127), 
                       Predict.3 = c(165, 1, 0))
knitr::kable(down_df, align = 'cccc', caption = "RF Confusion Matrices with and without Source Predictor", 
             booktabs=T) %>% 
  add_header_above(c("", "With Source" = 3, "Without Source" = 3)) %>%
kable_styling(latex_options = c("repeat_header", "scale_down"))
```

Second, we subset the processed training and testing data into two groups each: one with `Source = MapQuest` and one with `Source = Bing`. The few observations that were obtained from both APIs were dropped. Additionally, since only 8 observations from MapQuest had severity 3, those observations were dropped and the MapQuest model reduced to a two level outcome model. The breakdown of accidents per severity class in both of these training data subsets are output from the code below. We can see in both cases, the outcome remains unbalanced among the classes.

```{r}
bing.trn <- trn_data[c(which(trn_data$Source == "Bing")), ]
bing.test <- tst_data %>% dplyr::filter(Source == "Bing")
table(bing.trn$Severity_c)

mq.trn <- trn_data %>% dplyr::filter(Source == "MapQuest" & !(Severity_c == "3"))
mq.test <- tst_data %>% dplyr::filter(Source == "MapQuest" & !(Severity_c == "3"))
mq.test$Severity_c <- droplevels(mq.test$Severity_c)
mq.trn$Severity_c <- droplevels(mq.trn$Severity_c)
table(mq.trn$Severity_c)
```

The same code that was used to train Random Forest on the full training data was repeated on the source specific data. 

```{r, echo=FALSE}
img1 <-  grid::rasterGrob(as.raster(readPNG("varimp_bing.png")),
                            interpolate = FALSE)
img2 <-  grid::rasterGrob(as.raster(readPNG("varimp_mq.png")),
                          interpolate = FALSE)
gbm::grid.arrange(img1, img2, ncol = 2)
```

```{r, echo = FALSE}
# create table  
down_df <- data.frame(Reference = c("1", "2", "3"), 
                       Predict1 = c(1314, 364, 487), 
                       Predict2 = c(44, 20, 26), 
                       Predict3 = c(30, 13, 63), 
                       Predict.1 = c(32460, 267, "."), 
                       Predict.2 = c(2820, 5329, "."))
knitr::kable(down_df, align = 'cccc', caption = "Source Specific Confusion Matrices", 
             booktabs=T) %>% 
  add_header_above(c("", "Bing" = 3, "MapQuest" = 2)) %>%
kable_styling(latex_options = c("repeat_header", "scale_down"))
```


```{r, echo = FALSE}
# create table  
table_df <- data.frame(Method = c("Full Data No Source", "MapQuest Data", "Bing Data"), 
                       Training_Set_Size = c(75887, 59538, 15518), 
                       Accuracy = c(0.875, .924, .592), 
                       Kappa = c(0.510, 0.732, 0.0795), 
                       Time = c("12 minutes", "8  minutes", "2 minutes"))
knitr::kable(table_df, align = 'ccccc', caption = "Performance for Source Specific Random Forest Models", booktabs = TRUE) %>%
kable_styling(latex_options = c("repeat_header", "scale_down"))
```

# Discussion 

Although our accuracy values appear very high at first, we realize that API Source the data came from can greatly affect the results. This "batch effect" is undesirable when building a prediction model that should integrate these sources seamlessly. Aside from this issue, the most important variable in predicting car accident related traffic severity appears to be Interstate. In terms of our models, the Proportional Odds Model, Ordinal Net, Random Forest, and Ordinal Forest seem to perform similarly (in terms of accuracy) on our dataset with an outcome factor with 3 levels although the time to train varied widely. 

It is also clear that summary statistics such as accuracy/Cohen's Kappa do not always tell the whole story. Although they had similar accuracies, the confusion matrices showed us that the likelihood-based models on the full training data had no predictions for severity class 3, whereas the forest-based machine learning models did. This could be because of the Random Forest's ability to account for more complicated interactions between the variables, where our proportional odds model had no interaction terms included. 

We also discovered there is a "batch effect" in that the Source predictor is a very important variable in predicting severity class, although it would ideally have no effect on the outcome. It appears that the MapQuest data alone is more successful in predicting severity of other MapQuest based accidents.

A lot more data or different types of data may be needed to fully answer the questions this project aimed at answering, but this novel large scale and these types of analyses are a step in the right direction

## Limitations 

As mentioned earlier, our outcome variable severity was defined very vaguely from the two API sources. It is not clear exactly how these classes are derived and whether the "distance" between class 1 and 2 is the same as between class 2 and 3. This limitation contributes to our inability to truly understand the API Source effect on outcome.

There is also lack of information about factors that we believe may be important such as speed limit and type of vehicle(s) involved in the accident. If these were included, we may be able to build a model that captures the full picture. 

## Future Directions 

We assumed that the proportional odds assumption holds and also assumed there were no longitudinal effects in our data. Testing both of these assumptions would be needed moving forward to ensure our methods are sound. Additionally, adding a penalty feature to our own proportional odds model functions would be a next step, to see how that could potentially improve the likelihood-based models.

It would also be of interest to aggregate our outcome further into two severity classes by combining classes 2 and 3 in order compare models of this type to those we have already built.

In terms of the data, we would like to expand into more states and also incorporate local law enforcement data sets that contain information on the severity of car accidents in terms of injuries, fatalities, etc. One example source of this data is [Town of Cary Crash Data](https://data.world/townofcary/cpd-crash-incidents).

# Data Source 
The Car Accidents data set for North Carolina and the derived training and test data sets are provided in our project repository. Due to the size, we do not include the original nation-wide car accidents data set in our repository, but interested reader can find the dataset and related information on [Kaggle](https://www.kaggle.com/sobhanmoosavi/us-accidents) and the author's [Website](https://smoosavi.org/datasets/us_accidents).  
  
  
# References 
[1] Moosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, and Rajiv
      Ramnath. “A Countrywide Traffic Accident Dataset.”, 2019.
  
[2] Moosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, Radu
      Teodorescu, and Rajiv Ramnath. "Accident Risk Prediction based on Heterogeneous
      Sparse Data: New Dataset and Insights." In proceedings of the 27th ACM SIGSPATIAL
      International Conference on Advances in Geographic Information Systems, ACM, 2019.
      
[3] Bing documentation on traffic incident data: https://docs.microsoft.com/en-us/bingmaps/rest-services/traffic/traffic-incident-data
   
[4] MapQuest documentation on traffic incident data: https://developer.mapquest.com/documentation/traffic-api/incidents/get/#response_field-severity

[5] Levy, J.I., Buonocore, J.J. & von Stackelberg, K. Evaluation of the public health impacts of traffic congestion: a health risk assessment. Environ Health 9, 65 (2010). https://doi.org/10.1186/1476-069X-9-65

[6] Retallack, A. E., & Ostendorf, B. (2019). Current Understanding of the Effects of Congestion on Traffic Accidents. International journal of environmental research and public health, 16(18), 3400. https://doi.org/10.3390/ijerph16183400
      
*** add sources for methods, particularly where we derived the proportional odds likelihood/gradient from

